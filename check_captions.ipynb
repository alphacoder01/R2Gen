{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from abc import abstractmethod\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTester(object):\n",
    "    def __init__(self, model, criterion, metric_ftns, args):\n",
    "        self.args = args\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                            datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # setup GPU device if available, move model into configured device\n",
    "        self.device, device_ids = self._prepare_device(args.n_gpu)\n",
    "        self.model = model.to(self.device)\n",
    "        if len(device_ids) > 1:\n",
    "            self.model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.metric_ftns = metric_ftns\n",
    "\n",
    "        self.epochs = self.args.epochs\n",
    "        self.save_dir = self.args.save_dir\n",
    "\n",
    "        self._load_checkpoint(args.load)\n",
    "\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def plot(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _prepare_device(self, n_gpu_use):\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        if n_gpu_use > 0 and n_gpu == 0:\n",
    "            self.logger.warning(\n",
    "                \"Warning: There\\'s no GPU available on this machine,\" \"training will be performed on CPU.\")\n",
    "            n_gpu_use = 0\n",
    "        if n_gpu_use > n_gpu:\n",
    "            self.logger.warning(\n",
    "                \"Warning: The number of GPU\\'s configured to use is {}, but only {} are available \" \"on this machine.\".format(\n",
    "                    n_gpu_use, n_gpu))\n",
    "            n_gpu_use = n_gpu\n",
    "        device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')\n",
    "        list_ids = list(range(n_gpu_use))\n",
    "        return device, list_ids\n",
    "\n",
    "    def _load_checkpoint(self, load_path):\n",
    "        load_path = str(load_path)\n",
    "        self.logger.info(\"Loading checkpoint: {} ...\".format(load_path))\n",
    "        checkpoint = torch.load(load_path)\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(BaseTester):\n",
    "    def __init__(self, model, criterion, metric_ftns, args, test_dataloader):\n",
    "        super(Tester, self).__init__(model, criterion, metric_ftns, args)\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def test(self):\n",
    "        self.logger.info('Start to evaluate in the test set.')\n",
    "        log = dict()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_gts, test_res = [], []\n",
    "            for batch_idx, (images_id, images, reports_ids, reports_masks) in tqdm(enumerate(self.test_dataloader)):\n",
    "                images, reports_ids, reports_masks = images.to(self.device), reports_ids.to(\n",
    "                    self.device), reports_masks.to(self.device)\n",
    "                output = self.model(images, mode='sample')\n",
    "                reports = self.model.tokenizer.decode_batch(output.cpu().numpy())\n",
    "                ground_truths = self.model.tokenizer.decode_batch(reports_ids[:, 1:].cpu().numpy())\n",
    "                test_res.extend(reports)\n",
    "                test_gts.extend(ground_truths)\n",
    "            test_met = self.metric_ftns({i: [gt] for i, gt in enumerate(test_gts)},\n",
    "                                        {i: [re] for i, re in enumerate(test_res)})\n",
    "            log.update(**{'test_' + k: v for k, v in test_met.items()})\n",
    "            print(log)\n",
    "        return test_res, test_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "'image_dir': \"../../datasets/IUX_DATA/\",\n",
    "\"ann_path\" : \"../multiImageData.json\",\n",
    "\"dataset_name\": \"multi-image\",\n",
    "\"num_workers\": 10,\n",
    "\"batch_size\": 32,\n",
    "\"max_seq_length\":60,\n",
    "\"threshold\":3,\n",
    "\"visual_extractor\":\"resnet101\",\n",
    "\"visual_extractor_pretrained\":True,\n",
    "\"d_model\":512,\n",
    "\"--d_ff\":512,\n",
    "\"--d_vf\":2048,\n",
    "\"--num_heads\":8,\n",
    "\"--num_layers\":3,\n",
    "\"--dropout\":0.1,\n",
    "\"--logit_layers\":1,\n",
    "\"--bos_idx\":0,\n",
    "\"--eos_idx\":0,\n",
    "\"--pad_idx\":0,\n",
    "\"--use_bn\":0, \n",
    "\"--drop_prob_lm\":0.5,\n",
    "\"rm_num_slots\":3,\n",
    "\"rm_num_heads\":8,\n",
    "\"rm_d_model\":512,\n",
    "\"sample_method\":\"beam_search\",\n",
    "\"beam_size\":3,\n",
    "\"temperature\":1.0,\n",
    "\"sample_n\":1,\n",
    "\"group_size\":1,\n",
    "\"output_logsoftmax\":1,\n",
    "\"decoding_constraint\":0,\n",
    "\"block_trigrams\":1,\n",
    "\n",
    "\"n_gpu\":1,\n",
    "\"epochs\":100,\n",
    "\"save_dir\":\"results/iux-ray\",\n",
    "\"record_dir\":\"records/\",\n",
    "\"save_period\":1,\n",
    "\"monitor_mode\":\"max\",\n",
    "\"monitor_metric\":\"BLEU_4\",\n",
    "\n",
    "\"optim\":\"Adam\",\n",
    "\"lr_ve\":5e-5,\n",
    "\"lr_ed\":1e-4,\n",
    "\"weight_decay\":5e-5,\n",
    "\"amsgrad\":True,\n",
    "\"lr_scheduler\":\"StepLR\",\n",
    "\"step_size\":50,\n",
    "\"gamma\":0.1,\n",
    "\"seed\":2022,\n",
    "\"resume\":\"\",\n",
    "\"load\":\"/home/sweta/scratch/828-Project/R2Gen/results/augmented/current_checkpoint.pth\"\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "args = DotMap(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.tokenizers import Tokenizer\n",
    "from modules.dataloaders import R2DataLoader\n",
    "from modules.metrics import compute_scores\n",
    "from modules.loss import compute_loss\n",
    "from models.r2gen import R2GenModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): argument 'size' must be tuple of ints, but found element of type DotMap at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m test_dataloader \u001b[39m=\u001b[39m R2DataLoader(args, tokenizer, split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# build model architecture\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m R2GenModel(args, tokenizer)\n\u001b[1;32m      9\u001b[0m \u001b[39m# get function handles of loss and metrics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m criterion \u001b[39m=\u001b[39m compute_loss\n",
      "File \u001b[0;32m/nvme/scratch/sweta/828-Project/R2Gen/models/r2gen.py:15\u001b[0m, in \u001b[0;36mR2GenModel.__init__\u001b[0;34m(self, args, tokenizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_extractor \u001b[39m=\u001b[39m VisualExtractor(args)\n\u001b[0;32m---> 15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_decoder \u001b[39m=\u001b[39m EncoderDecoder(args, tokenizer)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mdataset_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmulti-image\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_multi\n",
      "File \u001b[0;32m/nvme/scratch/sweta/828-Project/R2Gen/modules/encoder_decoder.py:325\u001b[0m, in \u001b[0;36mEncoderDecoder.__init__\u001b[0;34m(self, args, tokenizer)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, args, tokenizer):\n\u001b[0;32m--> 325\u001b[0m     \u001b[39msuper\u001b[39;49m(EncoderDecoder, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(args, tokenizer)\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mnum_layers\n",
      "File \u001b[0;32m/nvme/scratch/sweta/828-Project/R2Gen/modules/att_model.py:60\u001b[0m, in \u001b[0;36mAttModel.__init__\u001b[0;34m(self, args, tokenizer)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_embed \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_embed \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39m(\n\u001b[1;32m     59\u001b[0m         ((nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_feat_size),) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bn \u001b[39melse\u001b[39;00m ()) \u001b[39m+\u001b[39m\n\u001b[0;32m---> 60\u001b[0m         (nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_feat_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_encoding_size),\n\u001b[1;32m     61\u001b[0m          nn\u001b[39m.\u001b[39mReLU(),\n\u001b[1;32m     62\u001b[0m          nn\u001b[39m.\u001b[39mDropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_prob_lm)) \u001b[39m+\u001b[39m\n\u001b[1;32m     63\u001b[0m         ((nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_encoding_size),) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bn \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m ())))\n",
      "File \u001b[0;32m~/scratch/miniconda3/envs/r2gen/lib/python3.8/site-packages/torch/nn/modules/linear.py:78\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 78\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mTensor(out_features, in_features))\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mTensor(out_features))\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type DotMap at pos 2"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(args)\n",
    "\n",
    "# create data loader\n",
    "test_dataloader = R2DataLoader(args, tokenizer, split='val', shuffle=False)\n",
    "\n",
    "# build model architecture\n",
    "model = R2GenModel(args, tokenizer)\n",
    "\n",
    "# get function handles of loss and metrics\n",
    "criterion = compute_loss\n",
    "metrics = compute_scores\n",
    "\n",
    "# build trainer and start to train\n",
    "tester = Tester(model, criterion, metrics, args, test_dataloader)\n",
    "tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('r2gen': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1239d90186665877fd16cf82a945889bf9ceef32665e873ddfdfd6a4888384cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
